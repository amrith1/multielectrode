{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f723e21",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'jax' has no attribute 'version' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/Lab/Users/praful/multielectrode/scripts/jupyter_notebooks/active_learning_loop_restarts_modelsel.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpeggyo/Volumes/Lab/Users/praful/multielectrode/scripts/jupyter_notebooks/active_learning_loop_restarts_modelsel.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmultielec_utils\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmutils\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpeggyo/Volumes/Lab/Users/praful/multielectrode/scripts/jupyter_notebooks/active_learning_loop_restarts_modelsel.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstatsmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msm\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bpeggyo/Volumes/Lab/Users/praful/multielectrode/scripts/jupyter_notebooks/active_learning_loop_restarts_modelsel.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpeggyo/Volumes/Lab/Users/praful/multielectrode/scripts/jupyter_notebooks/active_learning_loop_restarts_modelsel.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mjnp\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpeggyo/Volumes/Lab/Users/praful/multielectrode/scripts/jupyter_notebooks/active_learning_loop_restarts_modelsel.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmp\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/Lab/Development/miniconda-peggyo/envs/pvasi39/lib/python3.9/site-packages/jax/__init__.py:35\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdel\u001b[39;00m _cloud_tpu_init\n\u001b[1;32m     32\u001b[0m \u001b[39m# Confusingly there are two things named \"config\": the module and the class.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# We want the exported object to be the class, so we first import the module\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# to make sure a later import doesn't overwrite the class.\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m config \u001b[39mas\u001b[39;00m _config_module\n\u001b[1;32m     36\u001b[0m \u001b[39mdel\u001b[39;00m _config_module\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_src\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     39\u001b[0m   config \u001b[39mas\u001b[39;00m config,\n\u001b[1;32m     40\u001b[0m   enable_checks \u001b[39mas\u001b[39;00m enable_checks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m   transfer_guard_device_to_host \u001b[39mas\u001b[39;00m transfer_guard_device_to_host,\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m/Volumes/Lab/Development/miniconda-peggyo/envs/pvasi39/lib/python3.9/site-packages/jax/config.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2018 Google LLC\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[39m# TODO(phawkins): fix users of this alias and delete this file.\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_src\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n",
      "File \u001b[0;32m/Volumes/Lab/Development/miniconda-peggyo/envs/pvasi39/lib/python3.9/site-packages/jax/_src/config.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabsl\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_src\u001b[39;00m \u001b[39mimport\u001b[39;00m lib\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_src\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m \u001b[39mimport\u001b[39;00m jax_jit\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_src\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m \u001b[39mimport\u001b[39;00m transfer_guard_lib\n",
      "File \u001b[0;32m/Volumes/Lab/Development/miniconda-peggyo/envs/pvasi39/lib/python3.9/site-packages/jax/_src/lib/__init__.py:88\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[39mreturn\u001b[39;00m _jaxlib_version\n\u001b[1;32m     86\u001b[0m version_str \u001b[39m=\u001b[39m jaxlib\u001b[39m.\u001b[39mversion\u001b[39m.\u001b[39m__version__\n\u001b[1;32m     87\u001b[0m version \u001b[39m=\u001b[39m check_jaxlib_version(\n\u001b[0;32m---> 88\u001b[0m   jax_version\u001b[39m=\u001b[39mjax\u001b[39m.\u001b[39;49mversion\u001b[39m.\u001b[39m__version__,\n\u001b[1;32m     89\u001b[0m   jaxlib_version\u001b[39m=\u001b[39mjaxlib\u001b[39m.\u001b[39mversion\u001b[39m.\u001b[39m__version__,\n\u001b[1;32m     90\u001b[0m   minimum_jaxlib_version\u001b[39m=\u001b[39mjax\u001b[39m.\u001b[39mversion\u001b[39m.\u001b[39m_minimum_jaxlib_version)\n\u001b[1;32m     94\u001b[0m \u001b[39m# Before importing any C compiled modules from jaxlib, first import the CPU\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m# feature guard module to verify that jaxlib was compiled in a way that only\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m# uses instructions that are present on this machine.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjaxlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcpu_feature_guard\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mcpu_feature_guard\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'jax' has no attribute 'version' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # export OMP_NUM_THREADS=1\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" # export OPENBLAS_NUM_THREADS=1\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\" # export MKL_NUM_THREADS=1\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\" # export VECLIB_MAXIMUM_THREADS=1\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" # export NUMEXPR_NUM_THREADS=1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import src.fitting as fitting\n",
    "import src.multielec_utils as mutils\n",
    "import statsmodels.api as sm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import multiprocessing as mp\n",
    "from itertools import product\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# jax.config.update('jax_platform_name', 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad71c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_probs(x, w):\n",
    "    # w : site weights, n x d\n",
    "    # x : current levels, c x d\n",
    "    site_activations = jnp.dot(w, jnp.transpose(x)) # dimensions: n x c\n",
    "    p_sites = jax.nn.sigmoid(site_activations) # dimensions : n x c\n",
    "    p = 1 - jnp.prod(1 - p_sites, 0)  # dimensions: c\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d15748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path definitions\n",
    "ANALYSIS_BASE = \"/Volumes/Analysis\"\n",
    "MATFILE_BASE = \"/Volumes/Scratch/Users/praful/triplet_gsort_matfiles_20220420\"\n",
    "gsort_path = None\n",
    "gsort_path_1elec = \"/Volumes/Scratch/Users/praful/single_gsort_v2_30um_periphery-affinity_cosine\"\n",
    "\n",
    "dataset = \"2020-10-06-7\"\n",
    "estim = \"data003/data003-all\"\n",
    "estim_1elec = \"data001\"\n",
    "wnoise = \"kilosort_data000/data000\"\n",
    "electrical_path = os.path.join(ANALYSIS_BASE, dataset, estim)\n",
    "vis_datapath = os.path.join(ANALYSIS_BASE, dataset, wnoise)\n",
    "\n",
    "p = 3\n",
    "cell = 296\n",
    "\n",
    "X_expt_orig = mutils.get_stim_amps_newlv(electrical_path, p)\n",
    "# w_true = jnp.array([[-5.98518703, -5.73843676, -1.36037982, -0.05980741],\n",
    "#        [-5.98518703, -2.28047189, -2.93318102, -4.31001908],\n",
    "#        [-5.98518703,  5.39557745,  1.95279497,  1.8031558 ],\n",
    "#        [-5.98518703, -0.25671708,  2.89097144,  3.80746902]])\n",
    "w_true = jnp.array([[-5.68501006,  2.44477339,  3.23685565,  2.75812431],\n",
    "       [-5.66911426, -2.57285102, -3.49945348, -2.8179713 ]])\n",
    "\n",
    "X = jnp.array(sm.add_constant(X_expt_orig, has_constant='add'))\n",
    "p_true = activation_probs(X, w_true) # prob with each current level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.clear()\n",
    "ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "plt.xlim(-1.8, 1.8)\n",
    "plt.ylim(-1.8, 1.8)\n",
    "ax.set_zlim(-1.8, 1.8)\n",
    "ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "scat = ax.scatter(X_expt_orig[:, 0], \n",
    "            X_expt_orig[:, 1],\n",
    "            X_expt_orig[:, 2], marker='o', c=p_true, s=20, alpha=0.8, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3dd0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_spikes(p_true, t):\n",
    "    p_true, t = np.array(p_true), np.array(t).astype(int)\n",
    "    \n",
    "    p_empirical = []\n",
    "    for i in range(len(p_true)):\n",
    "        if t[i] == 0:\n",
    "            p_empirical += [0.5]\n",
    "        \n",
    "        else:\n",
    "            p_empirical += [np.mean(np.random.choice(np.array([0, 1]), \n",
    "                                                 p=np.array([1-p_true[i], p_true[i]]), \n",
    "                                                 size=t[i]))]\n",
    "        \n",
    "    return p_empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood(w, x, y, trials, l2_reg=0):\n",
    "    # x : current levels, c x d\n",
    "    # w : site weights, n x d\n",
    "    # y : empirical probability for each current level, c\n",
    "    # trials: number of trials at each current level, c\n",
    "    # l2_reg: l2 regularization penalty\n",
    "    # w = w.reshape(-1, x.shape[-1])  # dimensions: n x d\n",
    "    \n",
    "    p_model = activation_probs(x, w) # dimensions: c\n",
    "    p_model = jnp.clip(p_model, a_min=1e-5, a_max=1-1e-5)\n",
    "\n",
    "    trials = trials.astype(int)\n",
    "    \n",
    "    nll = -jnp.sum(trials * y * jnp.log(p_model) + trials * (1 - y) * jnp.log(1 - p_model))\n",
    "\n",
    "    penalty = l2_reg/2 * jnp.linalg.norm(w)**2\n",
    "\n",
    "    return nll + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeea455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_w(x, w, y, trials, l2_reg=0, zero_prob=0.01, step_size=0.0001, n_steps=100, wtol=1e-4):\n",
    "\n",
    "    m = len(w)\n",
    "    z = 1 - (1 - zero_prob)**(1/m)\n",
    "\n",
    "    @jax.jit\n",
    "    def update(x, w, y, trials, l2_reg):\n",
    "        grads = jax.grad(neg_log_likelihood)(w, x, y, trials, l2_reg=l2_reg)\n",
    "        return grads\n",
    "\n",
    "    losses = []\n",
    "    prev_w = w\n",
    "    for step in range(n_steps):\n",
    "        grad = update(x, w, y, trials, l2_reg)\n",
    "        w = w - step_size * grad\n",
    "        losses += [neg_log_likelihood(w, x, y, trials, l2_reg=l2_reg)]\n",
    "        w = w.at[:, 0].set(jnp.minimum(w[:, 0], np.log(z/(1-z))))\n",
    "\n",
    "        # print(step, jnp.linalg.norm(w - prev_w) / len(w.ravel()), jnp.linalg.norm(grad) / len(w.ravel()))\n",
    "        if jnp.linalg.norm(w - prev_w) / len(w.ravel()) <= wtol:\n",
    "            break\n",
    "        prev_w = w\n",
    "        \n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_info(x, w, y, t):\n",
    "    # x : current levels, c x d\n",
    "    # w : site weights, n x d\n",
    "    # y : empirical probability for each current level, c\n",
    "    # t: number of trials for each current level, c\n",
    "    \n",
    "    p_model = jnp.clip(activation_probs(x, w), a_min=1e-5, a_max=1-1e-5) # c\n",
    "    I_p = jnp.diag(t / (p_model * (1 - p_model)))   # c x c\n",
    "    J = jax.jacfwd(activation_probs, argnums=1)(x, w).reshape((len(x), w.shape[0]*w.shape[1]))\n",
    "    I_w = jnp.dot(jnp.dot(J.T, I_p), J) / len(x)\n",
    "    \n",
    "    loss = jnp.trace(J @ (jnp.linalg.inv(I_w) @ J.T))\n",
    "    # sign, logdet = jnp.linalg.slogdet(I_w)\n",
    "    # loss = -sign * logdet\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_proj_simplex(v, s=1):\n",
    "    \"\"\" Compute the Euclidean projection on a positive simplex\n",
    "    Solves the optimisation problem (using the algorithm from [1]):\n",
    "        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0 \n",
    "    Parameters\n",
    "    ----------\n",
    "    v: (n,) numpy array,\n",
    "       n-dimensional vector to project\n",
    "    s: int, optional, default: 1,\n",
    "       radius of the simplex\n",
    "    Returns\n",
    "    -------\n",
    "    w: (n,) numpy array,\n",
    "       Euclidean projection of v on the simplex\n",
    "    Notes\n",
    "    -----\n",
    "    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n",
    "    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n",
    "    However, this implementation still easily scales to millions of dimensions.\n",
    "    References\n",
    "    ----------\n",
    "    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n",
    "        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n",
    "        International Conference on Machine Learning (ICML 2008)\n",
    "        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n",
    "    \"\"\"\n",
    "    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n",
    "    n, = v.shape  # will raise ValueError if v is not 1-D\n",
    "    # check if we are already on the simplex\n",
    "    if v.sum() == s and np.alltrue(v >= 0):\n",
    "        # best projection: itself!\n",
    "        return v\n",
    "    # get the array of cumulative sums of a sorted (decreasing) copy of v\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    # get the number of > 0 components of the optimal solution\n",
    "    rho = np.nonzero(u * np.arange(1, n+1) > (cssv - s))[0][-1]\n",
    "    # compute the Lagrange multiplier associated to the simplex constraint\n",
    "    theta = (cssv[rho] - s) / (rho + 1.0)\n",
    "    # compute the projection by thresholding v using theta\n",
    "    w = (v - theta).clip(min=0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74fbbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_fisher(x, w, y, t_prev, t, reg=0, step_size=0.001, n_steps=100, step_cnt_decrement=5, reltol=5e-10):\n",
    "\n",
    "    @jax.jit\n",
    "    def update(x, w, y, t_prev, t):\n",
    "        fisher_lambda = lambda t, x, w, y, t_prev: fisher_info(x, w, y, t_prev + jnp.absolute(t)) + reg * jnp.sum(jnp.absolute(t))\n",
    "        grads = jax.grad(fisher_lambda)(t, x, w, y, t_prev)\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    losses = []\n",
    "    last_t = t\n",
    "    for step in range(n_steps):\n",
    "        grad = update(x, w, y, t_prev, t)\n",
    "        t = t - step_size * grad\n",
    "    \n",
    "        losses += [[fisher_info(x, w, y, t_prev + jnp.absolute(t)), \n",
    "                    jnp.sum(jnp.absolute(t)),\n",
    "                    fisher_info(x, w, y, t_prev + jnp.absolute(t)) + reg * jnp.sum(jnp.absolute(t))]]\n",
    "\n",
    "        curr_loss = fisher_info(x, w, y, t_prev + jnp.absolute(t)) + reg * jnp.sum(jnp.absolute(t))\n",
    "        last_loss = fisher_info(x, w, y, t_prev + jnp.absolute(last_t)) + reg * jnp.sum(jnp.absolute(last_t))\n",
    "        rel_decrease = jnp.absolute(curr_loss - last_loss) / last_loss\n",
    "        print(step, jnp.absolute(curr_loss - last_loss) / last_loss, jnp.linalg.norm(grad) / len(t))\n",
    "        if rel_decrease <= reltol:\n",
    "            break\n",
    "\n",
    "        last_t = t\n",
    "        if step % step_cnt_decrement == 0:\n",
    "            step_size = step_size * 0.99\n",
    "        \n",
    "    return np.array(losses), t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search_fisher(x, w, y, t_prev, t, reg, T_budget, step_size=0.001, n_steps=100, step_cnt_decrement=5, reltol=5e-10):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_AL(X, w_meas, p_true):\n",
    "    probs_pred = activation_probs(X, w_meas)\n",
    "    RMSE = jnp.sqrt(jnp.sum((probs_pred - p_true)**2) / len(X))\n",
    "\n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3593e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Initialization\n",
    "regs = [5, 3, 2, 1, 0.5, 0.1]\n",
    "ms = [2, 3, 4, 5]\n",
    "# num_restarts = 1\n",
    "init_size = 200\n",
    "budget = 500\n",
    "init_trials = 5\n",
    "\n",
    "# performance_stack = []\n",
    "# performance_stack_random = []\n",
    "\n",
    "# for m in range(num_restarts):\n",
    "    # Initialize amplitudes\n",
    "init_inds = np.random.choice(len(X), replace=False, size=init_size)\n",
    "\n",
    "# Initialize trials\n",
    "T_prev = jnp.zeros(len(X_expt_orig))\n",
    "T_prev = T_prev.at[init_inds].set(init_trials)\n",
    "T_prev_random = jnp.copy(T_prev)\n",
    "\n",
    "p_empirical = jnp.array(sample_spikes(p_true, T_prev))\n",
    "p_empirical_random = jnp.copy(p_empirical)\n",
    "\n",
    "# Initialize weights\n",
    "\n",
    "w_inits = []\n",
    "for m in ms:\n",
    "    w_init = jnp.array(np.random.normal(size=(m, X.shape[1])))\n",
    "    w_inits.append(w_init)\n",
    "\n",
    "w_inits_random = w_inits.copy()\n",
    "\n",
    "performances = []\n",
    "performances_random = []\n",
    "num_samples = []\n",
    "\n",
    "cnt = 0\n",
    "num_iters = 5\n",
    "\n",
    "while True:\n",
    "    reg = regs[cnt]\n",
    "    num_samples.append(np.sum(np.absolute(np.array(T_prev)).astype(int)))\n",
    "    sampled_inds = np.where(np.absolute(np.array(T_prev)).astype(int) > 0)[0]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.clear()\n",
    "    ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "    fig.add_axes(ax)\n",
    "    plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "    plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "    plt.xlim(-1.8, 1.8)\n",
    "    plt.ylim(-1.8, 1.8)\n",
    "    ax.set_zlim(-1.8, 1.8)\n",
    "    ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "    scat = ax.scatter(X_expt_orig[sampled_inds, 0], \n",
    "                X_expt_orig[sampled_inds, 1],\n",
    "                X_expt_orig[sampled_inds, 2], marker='o', c=p_empirical[sampled_inds], s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    sampled_inds_random = np.where(np.absolute(np.array(T_prev_random)).astype(int) > 0)[0]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.clear()\n",
    "    ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "    fig.add_axes(ax)\n",
    "    plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "    plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "    plt.xlim(-1.8, 1.8)\n",
    "    plt.ylim(-1.8, 1.8)\n",
    "    ax.set_zlim(-1.8, 1.8)\n",
    "    ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "    scat = ax.scatter(X_expt_orig[sampled_inds_random , 0], \n",
    "                X_expt_orig[sampled_inds_random , 1],\n",
    "                X_expt_orig[sampled_inds_random , 2], marker='o', c=p_empirical_random[sampled_inds_random], s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    weights_BIC = []\n",
    "    BICs = []\n",
    "    # Optimize w\n",
    "    for i in range(len(ms)):\n",
    "\n",
    "        losses, w_final = optimize_w(X, w_inits[i], p_empirical, T_prev, l2_reg=0.01, step_size=0.001, n_steps=3500)\n",
    "        p_pred = activation_probs(X, w_final)\n",
    "\n",
    "        BICs.append(len(w_final.ravel()) * jnp.log(len(X)) + 2 * losses[-1])\n",
    "        weights_BIC.append(w_final)\n",
    "\n",
    "        # fig = plt.figure()\n",
    "        # fig.clear()\n",
    "        # ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "        # fig.add_axes(ax)\n",
    "        # plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "        # plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "        # plt.xlim(-1.8, 1.8)\n",
    "        # plt.ylim(-1.8, 1.8)\n",
    "        # ax.set_zlim(-1.8, 1.8)\n",
    "        # ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "        # scat = ax.scatter(X_expt_orig[:, 0], \n",
    "        #             X_expt_orig[:, 1],\n",
    "        #             X_expt_orig[:, 2], marker='o', c=p_pred, s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "\n",
    "        # plt.show()\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(losses)\n",
    "        # plt.axhline(neg_log_likelihood(w_true, X, p_empirical, T_prev), linestyle='--', c='k')\n",
    "        # plt.show()\n",
    "\n",
    "        w_inits[i] = w_final\n",
    "    w_final = weights_BIC[jnp.argmin(jnp.array(BICs))]\n",
    "    performance = get_performance_AL(X, w_final, p_true)\n",
    "    performances.append(performance)\n",
    "\n",
    "    print(BICs, w_final)\n",
    "\n",
    "    weights_BIC_random = []\n",
    "    BICs_random = []\n",
    "    # Optimize w\n",
    "    for i in range(len(ms)):\n",
    "\n",
    "        losses_random, w_final_random = optimize_w(X, w_inits_random[i], p_empirical_random, T_prev_random, l2_reg=0.01, step_size=0.001, n_steps=3500)\n",
    "        p_pred_random = activation_probs(X, w_final_random)\n",
    "\n",
    "        BICs_random.append(len(w_final_random.ravel()) * jnp.log(len(X)) + 2 * losses_random[-1])\n",
    "        weights_BIC_random.append(w_final_random)\n",
    "\n",
    "        # fig = plt.figure()\n",
    "        # fig.clear()\n",
    "        # ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "        # fig.add_axes(ax)\n",
    "        # plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "        # plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "        # plt.xlim(-1.8, 1.8)\n",
    "        # plt.ylim(-1.8, 1.8)\n",
    "        # ax.set_zlim(-1.8, 1.8)\n",
    "        # ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "        # scat = ax.scatter(X_expt_orig[:, 0], \n",
    "        #             X_expt_orig[:, 1],\n",
    "        #             X_expt_orig[:, 2], marker='o', c=p_pred_random, s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "\n",
    "        # plt.show()\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(losses_random)\n",
    "        # plt.axhline(neg_log_likelihood(w_true, X, p_empirical_random, T_prev_random), linestyle='--', c='k')\n",
    "        # plt.show()\n",
    "\n",
    "        w_inits_random[i] = w_final_random\n",
    "    w_final_random = weights_BIC_random[jnp.argmin(jnp.array(BICs_random))]\n",
    "    performance_random = get_performance_AL(X, w_final_random, p_true)\n",
    "    performances_random.append(performance_random)\n",
    "\n",
    "    print(BICs_random, w_final_random)\n",
    "    print(performance, performance_random)\n",
    "\n",
    "    if cnt >= num_iters:\n",
    "        break\n",
    "\n",
    "    # explore = performance\n",
    "    # explore_batch = 0#int(explore * budget) * 2\n",
    "    # exploit_batch = budget - explore_batch\n",
    "\n",
    "    T_new_init = jnp.zeros(len(T_prev)) + 1\n",
    "    losses, t_final = optimize_fisher(X, w_final, p_empirical, T_prev, T_new_init, reg=reg, step_size=0.01,\n",
    "                                                    n_steps=3000, T_budget=budget)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axs[0].plot(losses[:, 0])\n",
    "    axs[0].set_ylabel('Fisher Loss (A-optimality)')\n",
    "    axs[1].plot(losses[:, 1])\n",
    "    axs[1].set_ylabel('Total Trials')\n",
    "    axs[2].plot(losses[:, 2])\n",
    "    axs[2].set_ylabel('Regularized Loss, reg=' + str(reg))\n",
    "\n",
    "    fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "    plt.show()\n",
    "\n",
    "    # random_draws_explore = np.random.choice(len(X), size=explore_batch)\n",
    "    # T_new_explore = jnp.array(np.bincount(random_draws_explore, minlength=len(X)))\n",
    "\n",
    "    T_new = jnp.round(jnp.absolute(t_final), 0)#(t_final + T_new_explore), 0)\n",
    "    print(jnp.sum(T_new))\n",
    "    plt.figure()\n",
    "    plt.plot(T_new)\n",
    "    plt.show()\n",
    "\n",
    "    p_new = jnp.array(sample_spikes(p_true, T_new))\n",
    "\n",
    "    p_tmp = (p_new * T_new + p_empirical * T_prev) / (T_prev + T_new)\n",
    "    T_tmp = T_prev + T_new\n",
    "    p_tmp = p_tmp.at[jnp.isnan(p_tmp)].set(0.5)\n",
    "\n",
    "    p_empirical = p_tmp\n",
    "    T_prev = T_tmp\n",
    "    print(jnp.sum(T_tmp))\n",
    "\n",
    "    random_draws = np.random.choice(len(X), size=int(jnp.sum(T_new)))\n",
    "    T_new_random = jnp.array(np.bincount(random_draws, minlength=len(X))).astype(int)\n",
    "    p_new_random = jnp.array(sample_spikes(p_true, T_new_random))\n",
    "    \n",
    "    p_tmp_random = (p_new_random * T_new_random + p_empirical_random * T_prev_random) / (T_prev_random + T_new_random)\n",
    "    T_tmp_random = T_prev_random + T_new_random\n",
    "    p_tmp_random = p_tmp_random.at[jnp.isnan(p_tmp_random)].set(0.5)\n",
    "\n",
    "    p_empirical_random = p_tmp_random\n",
    "    T_prev_random = T_tmp_random\n",
    "\n",
    "    cnt += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a23e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61de6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(num_samples, performances, label='Active Learning', linewidth=4)\n",
    "plt.plot(num_samples, performances_random, label='Random Baseline', linewidth=4)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlabel('Number of Trials Sampled', fontsize=24)\n",
    "plt.ylabel(r'RMSE', fontsize=24)\n",
    "plt.legend(fontsize=20)\n",
    "# plt.ylim(0.36, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b28388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pvasi39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a49c36f17b22be80bca1a531e420dccca9dd908a69799277bc977a4a0d3e51d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
