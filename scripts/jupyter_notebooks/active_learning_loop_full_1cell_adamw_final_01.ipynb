{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f723e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # export OMP_NUM_THREADS=1\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" # export OPENBLAS_NUM_THREADS=1\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\" # export MKL_NUM_THREADS=1\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\" # export VECLIB_MAXIMUM_THREADS=1\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" # export NUMEXPR_NUM_THREADS=1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import src.fitting as fitting\n",
    "import src.multielec_utils as mutils\n",
    "import statsmodels.api as sm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from scipy.io import savemat, loadmat\n",
    "import optax\n",
    "from matplotlib import cm\n",
    "from copy import copy\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad71c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_probs(x, w):\n",
    "    # w : site weights, n x d\n",
    "    # x : current levels, c x d\n",
    "    site_activations = jnp.dot(w, jnp.transpose(x)) # dimensions: n x c\n",
    "    p_sites = jax.nn.sigmoid(site_activations) # dimensions : n x c\n",
    "    p = 1 - jnp.prod(1 - p_sites, 0)  # dimensions: c\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2962c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood(w, X, y, l2_reg=0):\n",
    "    # x : current levels, c x d\n",
    "    # w : site weights, n x d\n",
    "    # y : empirical probability for each current level, c\n",
    "    # trials: number of trials at each current level, c\n",
    "    # l2_reg: l2 regularization penalty\n",
    "    # w = w.reshape(-1, x.shape[-1])  # dimensions: n x d\n",
    "    \n",
    "    # Get predicted probability of spike using current parameters\n",
    "    yPred = activation_probs(X, w)\n",
    "    yPred = jnp.clip(yPred, a_min=1e-5, a_max=1-1e-5)\n",
    "\n",
    "    NLL = -jnp.sum(y * jnp.log(yPred) + (1 - y) * jnp.log(1 - yPred))   # negative log likelihood for logistic\n",
    "\n",
    "    penalty = l2_reg/2 * jnp.linalg.norm(w)**2\n",
    "    # print(NLL, penalty)\n",
    "\n",
    "    return NLL + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5634d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_w(w, X, y, l2_reg=0, zero_prob=0.01, step_size=0.0001, n_steps=100, wtol=5e-5, step_cnt_decrement=5):\n",
    "\n",
    "    m = len(w)\n",
    "    z = 1 - (1 - zero_prob)**(1/m)\n",
    "\n",
    "    optimizer = optax.adamw(step_size)\n",
    "    opt_state = optimizer.init(w)\n",
    "\n",
    "    @jax.jit\n",
    "    def update(w, X, y, l2_reg):\n",
    "        grads = jax.grad(neg_log_likelihood)(w, X, y, l2_reg=l2_reg)\n",
    "        return grads\n",
    "\n",
    "    losses = []\n",
    "    prev_w = w\n",
    "    for step in range(n_steps):\n",
    "        grads = update(w, X, y, l2_reg)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params=w)\n",
    "        w = optax.apply_updates(w, updates)\n",
    "\n",
    "        # grad = update(w, X, y, l2_reg)\n",
    "        # w = w - step_size * grad\n",
    "        losses += [neg_log_likelihood(w, X, y, l2_reg=l2_reg)]\n",
    "        w = w.at[:, 0].set(jnp.minimum(w[:, 0], np.log(z/(1-z))))\n",
    "\n",
    "        # print(step, jnp.linalg.norm(w - prev_w) / len(w.ravel()), jnp.linalg.norm(grad) / len(w.ravel()))\n",
    "        if jnp.linalg.norm(w - prev_w) / len(w.ravel()) <= wtol:\n",
    "            break\n",
    "        prev_w = w\n",
    "        \n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ab2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_surface_McF(X, y, w_inits, R2_thresh=0.02, l2_reg=0.01, w_step_size=0.001, n_steps=3500, plot=False, random_state=None):\n",
    "    \n",
    "    ybar = jnp.mean(y)\n",
    "    beta_null = jnp.log(ybar / (1 - ybar))\n",
    "    null_weights = jnp.concatenate((jnp.array([beta_null]), jnp.zeros(X.shape[-1]-1)))\n",
    "    nll_null = neg_log_likelihood(null_weights, X, y, l2_reg=l2_reg)\n",
    "    print(nll_null)\n",
    "    \n",
    "    losses, w_final = optimize_w(w_inits[0], X, y, l2_reg=l2_reg, step_size=w_step_size, n_steps=n_steps)\n",
    "    last_opt = w_final\n",
    "    last_R2 = 1 - losses[-1] / nll_null\n",
    "    w_inits[0] = w_final\n",
    "    print(last_R2, losses[-1])\n",
    "\n",
    "    if plot:\n",
    "        p_pred_all = activation_probs(X_all, w_final)\n",
    "        fig = plt.figure()\n",
    "        fig.clear()\n",
    "        ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "        fig.add_axes(ax)\n",
    "        plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "        plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "        plt.xlim(-1.8, 1.8)\n",
    "        plt.ylim(-1.8, 1.8)\n",
    "        ax.set_zlim(-1.8, 1.8)\n",
    "        ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "        scat = ax.scatter(X_all[:, 1], \n",
    "                    X_all[:, 2],\n",
    "                    X_all[:, 3], marker='o', c=p_pred_all, s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(losses)\n",
    "        plt.show()\n",
    "\n",
    "    for i in range(1, len(w_inits)):\n",
    "        losses, w_final = optimize_w(w_inits[i], X, y, l2_reg=l2_reg, step_size=w_step_size, n_steps=n_steps)\n",
    "        w_inits[i] = w_final\n",
    "\n",
    "        if plot:\n",
    "            p_pred_all = activation_probs(X_all, w_final)\n",
    "            fig = plt.figure()\n",
    "            fig.clear()\n",
    "            ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "            fig.add_axes(ax)\n",
    "            plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "            plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "            plt.xlim(-1.8, 1.8)\n",
    "            plt.ylim(-1.8, 1.8)\n",
    "            ax.set_zlim(-1.8, 1.8)\n",
    "            ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "            scat = ax.scatter(X_all[:, 1], \n",
    "                        X_all[:, 2],\n",
    "                        X_all[:, 3], marker='o', c=p_pred_all, s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(losses)\n",
    "            plt.show()\n",
    "\n",
    "        new_opt = w_final\n",
    "        new_R2 = 1 - losses[-1] / nll_null\n",
    "        print(new_R2, losses[-1])\n",
    "        if new_R2 - last_R2 <= R2_thresh:\n",
    "            break\n",
    "\n",
    "        last_opt = new_opt\n",
    "        last_R2 = new_R2\n",
    "\n",
    "    w_final = last_opt\n",
    "    \n",
    "    if plot:\n",
    "        p_pred_all = activation_probs(X_all, w_final)\n",
    "        fig = plt.figure()\n",
    "        fig.clear()\n",
    "        ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "        fig.add_axes(ax)\n",
    "        plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "        plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "        plt.xlim(-1.8, 1.8)\n",
    "        plt.ylim(-1.8, 1.8)\n",
    "        ax.set_zlim(-1.8, 1.8)\n",
    "        ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "        scat = ax.scatter(X_all[:, 1], \n",
    "                    X_all[:, 2],\n",
    "                    X_all[:, 3], marker='o', c=p_pred_all, s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # fig = plt.figure()\n",
    "        # fig.clear()\n",
    "        # ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "        # fig.add_axes(ax)\n",
    "        # plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "        # plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "        # plt.xlim(-1.8, 1.8)\n",
    "        # plt.ylim(-1.8, 1.8)\n",
    "        # ax.set_zlim(-1.8, 1.8)\n",
    "        # ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "        # scat = ax.scatter(X[:, 1], \n",
    "        #             X[:, 2],\n",
    "        #             X[:, 3], marker='o', c=probs, s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "\n",
    "        # plt.show()\n",
    "\n",
    "    return w_final, w_inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d15748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path definitions\n",
    "ANALYSIS_BASE = \"/Volumes/Analysis\"\n",
    "MATFILE_BASE = \"/Volumes/Scratch/Users/praful/triplet_gsort_matfiles_20220420\"\n",
    "gsort_path = None\n",
    "\n",
    "dataset = \"2020-10-06-7\"\n",
    "estim = \"data003/data003-all\"\n",
    "wnoise = \"kilosort_data000/data000\"\n",
    "electrical_path = os.path.join(ANALYSIS_BASE, dataset, estim)\n",
    "vis_datapath = os.path.join(ANALYSIS_BASE, dataset, wnoise)\n",
    "\n",
    "ms = [2, 3, 4, 5]\n",
    "l2_reg = 0\n",
    "w_step_size = 0.1\n",
    "# folds = 5\n",
    "n_steps = 1000\n",
    "# train_size = 0.8\n",
    "\n",
    "# method = 'L-BFGS-B'\n",
    "# jac = fitting.negLL_hotspot_jac\n",
    "# l2_reg = 0.1\n",
    "R2_thresh = 0.002\n",
    "\n",
    "p = 3\n",
    "cell = 296\n",
    "\n",
    "X_expt_orig, probs_orig, T_orig = mutils.loadNewLVData(electrical_path, gsort_path, dataset, estim, wnoise, p, cell,\n",
    "                                        load_from_mat=True, \n",
    "                                        MATFILE_BASE=MATFILE_BASE)\n",
    "X_all = jnp.array(sm.add_constant(mutils.get_stim_amps_newlv(electrical_path, p), has_constant='add'))\n",
    "X_expt, probs, T = mutils.triplet_cleaning(X_expt_orig, probs_orig, T_orig, electrical_path, p)\n",
    "# w_true = jnp.array(fitting.fit_triplet_surface(X_expt, probs, T, method=method, jac=jac, starting_m=2, max_sites=5, reg_method='l2', \n",
    "#                                         reg=l2_reg, R2_thresh=R2_thresh, verbose=True))\n",
    "X_bin, y_bin = fitting.convertToBinaryClassifier(probs, T, X_expt)\n",
    "X_bin = jnp.array(X_bin)\n",
    "y_bin = jnp.array(y_bin)\n",
    "\n",
    "w_inits = []\n",
    "for m in ms:\n",
    "    w_init = jnp.array(np.random.normal(size=(m, X_bin.shape[1])))\n",
    "    w_inits.append(w_init)\n",
    "    \n",
    "w_true, _ =  fit_surface_McF(X_bin, y_bin, w_inits, R2_thresh=R2_thresh, l2_reg=l2_reg, w_step_size=w_step_size, n_steps=n_steps, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f42b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_true)\n",
    "X_expt_orig = mutils.get_stim_amps_newlv(electrical_path, p)\n",
    "p_true = activation_probs(jnp.array(sm.add_constant(X_expt_orig, has_constant='add')), w_true) # prob with each current level\n",
    "X = jnp.array(X_expt_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.clear()\n",
    "ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "plt.xlim(-1.8, 1.8)\n",
    "plt.ylim(-1.8, 1.8)\n",
    "ax.set_zlim(-1.8, 1.8)\n",
    "ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "scat = ax.scatter(X_expt_orig[:, 0], \n",
    "            X_expt_orig[:, 1],\n",
    "            X_expt_orig[:, 2], marker='o', c=p_true, s=20, alpha=0.8, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3dd0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_spikes(p_true, t):\n",
    "    p_true, t = np.array(p_true), np.array(t).astype(int)\n",
    "    \n",
    "    p_empirical = []\n",
    "    for i in range(len(p_true)):\n",
    "        if t[i] == 0:\n",
    "            p_empirical += [0.5]\n",
    "        \n",
    "        else:\n",
    "            p_empirical += [np.mean(np.random.choice(np.array([0, 1]), \n",
    "                                                 p=np.array([1-p_true[i], p_true[i]]), \n",
    "                                                 size=t[i]))]\n",
    "        \n",
    "    return p_empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_info(x, w, t):\n",
    "    # x : current levels, c x d\n",
    "    # w : site weights, n x d\n",
    "    # y : empirical probability for each current level, c\n",
    "    # t: number of trials for each current level, c\n",
    "    \n",
    "    p_model = jnp.clip(activation_probs(x, w), a_min=1e-5, a_max=1-1e-5) # c\n",
    "    I_p = jnp.diag(t / (p_model * (1 - p_model)))   # c x c\n",
    "    J = jax.jacfwd(activation_probs, argnums=1)(x, w).reshape((len(x), w.shape[0]*w.shape[1]))\n",
    "    I_w = jnp.dot(jnp.dot(J.T, I_p), J) / len(x)\n",
    "    \n",
    "    loss = jnp.trace(J @ (jnp.linalg.inv(I_w) @ J.T))\n",
    "    # sign, logdet = jnp.linalg.slogdet(I_w)\n",
    "    # loss = -sign * logdet\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74fbbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_fisher(x, w, t_prev, t, reg=0, step_size=0.001, n_steps=100, reltol=-np.inf, T_budget=5000, step_cnt_decrement=5):\n",
    "\n",
    "    optimizer = optax.adamw(step_size)\n",
    "    opt_state = optimizer.init(t)\n",
    "\n",
    "    @jax.jit\n",
    "    def update(x, w, t_prev, t):\n",
    "        fisher_lambda = lambda t, x, w, t_prev: fisher_info(x, w, t_prev + jnp.absolute(t))  + reg * jnp.absolute(jnp.sum(jnp.absolute(t)) - T_budget)\n",
    "        # fisher_lambda = lambda t, x, w, t_prev: fisher_info(x, w, t_prev + jnp.absolute(t))  + reg * jnp.sum(jnp.absolute(t))\n",
    "\n",
    "        grads = jax.grad(fisher_lambda)(t, x, w, t_prev)\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    losses = []\n",
    "    for step in range(n_steps):\n",
    "        grads = update(x, w, t_prev, t)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params=t)\n",
    "        t = optax.apply_updates(t, updates)\n",
    "        # grad = update(x, w, t_prev, t)\n",
    "        # t = t - step_size * grad\n",
    "    \n",
    "        losses += [[fisher_info(x, w, t_prev + jnp.absolute(t)), \n",
    "                    jnp.sum(jnp.absolute(t)),\n",
    "                    # fisher_info(x, w, t_prev + jnp.absolute(t)) + reg * jnp.sum(jnp.absolute(t))]]\n",
    "                    fisher_info(x, w, t_prev + jnp.absolute(t)) + reg * jnp.absolute(jnp.sum(jnp.absolute(t)) - T_budget)]]\n",
    "        \n",
    "        # if step % step_cnt_decrement == 0:\n",
    "        #     step_size = step_size * 0.95\n",
    "\n",
    "    return np.array(losses), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_AL(X, w_meas, p_true):\n",
    "    probs_pred = activation_probs(X, w_meas)\n",
    "    RMSE = jnp.sqrt(jnp.sum((probs_pred - p_true)**2) / len(X))\n",
    "\n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3593e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "total_budget = 10000\n",
    "num_iters = 5\n",
    "budget = int(total_budget / num_iters)\n",
    "reg = 20    #np.flip(np.logspace(-5, 3, 100000, base=2))\n",
    "num_restarts = 1\n",
    "# w_step_size = 1\n",
    "l2_reg = 0\n",
    "T_step_size = 0.01 #0.01\n",
    "T_n_steps = 5000\n",
    "\n",
    "init_size = 200\n",
    "init_trials = 5\n",
    "\n",
    "performance_stack = []\n",
    "performance_stack_random = []\n",
    "num_samples_stack = []\n",
    "\n",
    "for restart in range(num_restarts):\n",
    "    print('Restart', restart + 1)\n",
    "    # Initialize amplitudes\n",
    "    init_inds = np.random.choice(len(X), replace=False, size=init_size)\n",
    "\n",
    "    # Initialize trials\n",
    "    T_prev = jnp.zeros(len(X_expt_orig))\n",
    "    T_prev = T_prev.at[init_inds].set(init_trials)\n",
    "    T_prev_random = jnp.copy(T_prev)\n",
    "\n",
    "    p_empirical = jnp.array(sample_spikes(p_true, T_prev))\n",
    "    p_empirical_random = jnp.copy(p_empirical)\n",
    "\n",
    "    performances = []\n",
    "    performances_random = []\n",
    "    num_samples = []\n",
    "\n",
    "    w_inits = []\n",
    "    for m in ms:\n",
    "        w_init = jnp.array(np.random.normal(size=(m, X.shape[1]+1)))\n",
    "        w_inits.append(w_init)\n",
    "\n",
    "    w_inits_random = copy(w_inits)\n",
    "    cnt = 0\n",
    "\n",
    "    while True:\n",
    "        num_samples.append(np.sum(np.absolute(np.array(T_prev)).astype(int)))\n",
    "        sampled_inds = np.where(np.absolute(np.array(T_prev)).astype(int) > 0)[0]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        fig.clear()\n",
    "        ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "        fig.add_axes(ax)\n",
    "        plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "        plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "        plt.xlim(-1.8, 1.8)\n",
    "        plt.ylim(-1.8, 1.8)\n",
    "        ax.set_zlim(-1.8, 1.8)\n",
    "        ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "        scat = ax.scatter(X_expt_orig[sampled_inds, 0], \n",
    "                    X_expt_orig[sampled_inds, 1],\n",
    "                    X_expt_orig[sampled_inds, 2], marker='o', c=p_empirical[sampled_inds], s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "        # scat = ax.scatter(X_expt_orig[sampled_inds, 0], \n",
    "        #             X_expt_orig[sampled_inds, 1],\n",
    "        #             X_expt_orig[sampled_inds, 2], marker='o', c=T_prev[sampled_inds], s=20, cmap=cm.jet, vmin=0, vmax=20)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        sampled_inds_random = np.where(np.absolute(np.array(T_prev_random)).astype(int) > 0)[0]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        fig.clear()\n",
    "        ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "        fig.add_axes(ax)\n",
    "        plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "        plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "        plt.xlim(-1.8, 1.8)\n",
    "        plt.ylim(-1.8, 1.8)\n",
    "        ax.set_zlim(-1.8, 1.8)\n",
    "        ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "        scat = ax.scatter(X_expt_orig[sampled_inds_random , 0], \n",
    "                    X_expt_orig[sampled_inds_random , 1],\n",
    "                    X_expt_orig[sampled_inds_random , 2], marker='o', c=p_empirical_random[sampled_inds_random], s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "\n",
    "        # scat = ax.scatter(X_expt_orig[sampled_inds_random , 0], \n",
    "        #     X_expt_orig[sampled_inds_random , 1],\n",
    "        #     X_expt_orig[sampled_inds_random , 2], marker='o', c=T_prev_random[sampled_inds_random], s=20, cmap=cm.jet, vmin=0, vmax=20)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        good_inds = jnp.where((p_empirical[sampled_inds] > 0) & (p_empirical[sampled_inds] < 1))[0]\n",
    "        X_bin, y_bin = fitting.convertToBinaryClassifier(np.array(p_empirical[sampled_inds][good_inds]), \n",
    "                                                         np.array(T_prev[sampled_inds][good_inds], dtype=int),\n",
    "                                                         np.array(X[sampled_inds][good_inds]))\n",
    "        X_bin = jnp.array(X_bin)\n",
    "        y_bin = jnp.array(y_bin)\n",
    "\n",
    "        w_final, w_inits =  fit_surface_McF(X_bin, y_bin, w_inits, R2_thresh=R2_thresh, \n",
    "                                            l2_reg=l2_reg, w_step_size=w_step_size, n_steps=n_steps, plot=False)\n",
    "        \n",
    "        # w_final = jnp.array(fitting.fit_triplet_surface(np.array(X[sampled_inds]), np.array(p_empirical[sampled_inds]), np.array(T_prev[sampled_inds], dtype=int), \n",
    "        #                                                     method=method, jac=jac, starting_m=2, max_sites=5, reg_method='l2', reg=l2_reg,\n",
    "        #                                                     R2_thresh=R2_thresh,\n",
    "        #                                                     verbose=True))\n",
    "        print(w_final)\n",
    "\n",
    "        if cnt == 0:\n",
    "            T_new_init = jnp.zeros(len(T_prev)) + 1\n",
    "            w_final_random = jnp.copy(w_final)\n",
    "            w_inits_random = copy(w_inits)\n",
    "        else:\n",
    "            T_new_init = t_final\n",
    "            good_inds_random = jnp.where((p_empirical_random[sampled_inds_random] > 0) & (p_empirical_random[sampled_inds_random] < 1))[0]\n",
    "            X_bin, y_bin = fitting.convertToBinaryClassifier(np.array(p_empirical_random[sampled_inds_random][good_inds_random]), \n",
    "                                                         np.array(T_prev_random[sampled_inds_random][good_inds_random], dtype=int),\n",
    "                                                         np.array(X[sampled_inds_random][good_inds_random]))\n",
    "            X_bin = jnp.array(X_bin)\n",
    "            y_bin = jnp.array(y_bin)\n",
    "\n",
    "            w_final_random, w_inits_random =  fit_surface_McF(X_bin, y_bin, w_inits_random, R2_thresh=R2_thresh, \n",
    "                                                l2_reg=l2_reg, w_step_size=w_step_size, n_steps=n_steps, plot=False)\n",
    "\n",
    "            # w_final_random = jnp.array(fitting.fit_triplet_surface(np.array(X[sampled_inds_random]), np.array(p_empirical_random[sampled_inds_random]), np.array(T_prev_random[sampled_inds_random], dtype=int), \n",
    "            #                                                         method=method, jac=jac, starting_m=2, max_sites=5, reg_method='l2', reg=l2_reg,\n",
    "            #                                                         R2_thresh=R2_thresh,\n",
    "            #                                                         verbose=True))\n",
    "        print(w_final_random)\n",
    "\n",
    "        performance = get_performance_AL(jnp.array(sm.add_constant(X, has_constant='add')), w_final, p_true)\n",
    "        performances.append(performance)\n",
    "        \n",
    "        performance_random = get_performance_AL(jnp.array(sm.add_constant(X, has_constant='add')), w_final_random, p_true)\n",
    "        performances_random.append(performance_random)\n",
    "\n",
    "        print(performance, performance_random)\n",
    "        # print('\\n')\n",
    "\n",
    "        if cnt >= num_iters:\n",
    "            break\n",
    "        \n",
    "        losses, t_final = optimize_fisher(jnp.array(sm.add_constant(X, has_constant='add')), w_final, T_prev, T_new_init, reg=reg, step_size=T_step_size, n_steps=T_n_steps, T_budget=budget)\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        axs[0].plot(losses[:, 0])\n",
    "        axs[0].set_ylabel('Fisher Loss (A-optimality)')\n",
    "        axs[1].plot(losses[:, 1])\n",
    "        axs[1].set_ylabel('Total Trials')\n",
    "        axs[2].plot(losses[:, 2])\n",
    "        axs[2].set_ylabel('Regularized Loss, reg=' + str(reg))\n",
    "\n",
    "        fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "        plt.show()\n",
    "\n",
    "        T_new = jnp.round(jnp.absolute(t_final), 0)#(t_final + T_new_explore), 0)\n",
    "        print(jnp.sum(T_new))\n",
    "        plt.figure()\n",
    "        plt.plot(T_new)\n",
    "        plt.show()\n",
    "\n",
    "        if jnp.sum(T_new) < budget:\n",
    "            random_extra = np.random.choice(len(X), size=int(budget - jnp.sum(T_new)), \n",
    "                                            p=np.array(jnp.absolute(t_final))/np.sum(np.array(jnp.absolute(t_final))))\n",
    "            T_new_extra = jnp.array(np.bincount(random_extra, minlength=len(X))).astype(int)\n",
    "            T_new = T_new + T_new_extra\n",
    "            \n",
    "            print(jnp.sum(T_new))\n",
    "            plt.figure()\n",
    "            plt.plot(T_new)\n",
    "            plt.show()\n",
    "\n",
    "        p_new = jnp.array(sample_spikes(p_true, T_new))\n",
    "\n",
    "        p_tmp = (p_new * T_new + p_empirical * T_prev) / (T_prev + T_new)\n",
    "        T_tmp = T_prev + T_new\n",
    "        p_tmp = p_tmp.at[jnp.isnan(p_tmp)].set(0.5)\n",
    "\n",
    "        p_empirical = p_tmp\n",
    "        T_prev = T_tmp\n",
    "\n",
    "        # print(jnp.sum(T_tmp))\n",
    "\n",
    "        random_draws = np.random.choice(len(X), size=int(jnp.sum(T_new)))\n",
    "        T_new_random = jnp.array(np.bincount(random_draws, minlength=len(X))).astype(int)\n",
    "        p_new_random = jnp.array(sample_spikes(p_true, T_new_random))\n",
    "        \n",
    "        p_tmp_random = (p_new_random * T_new_random + p_empirical_random * T_prev_random) / (T_prev_random + T_new_random)\n",
    "        T_tmp_random = T_prev_random + T_new_random\n",
    "        p_tmp_random = p_tmp_random.at[jnp.isnan(p_tmp_random)].set(0.5)\n",
    "\n",
    "        p_empirical_random = p_tmp_random\n",
    "        T_prev_random = T_tmp_random\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "        # input()\n",
    "    \n",
    "    performance_stack.append(performances)\n",
    "    performance_stack_random.append(performances_random)\n",
    "    num_samples_stack.append(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "savemat('sampling_map_p3_n296.mat', {'X': np.array(X_expt_orig),\n",
    "                                     'p_empirical': np.array(p_empirical),\n",
    "                                     'sampled_inds': np.array(sampled_inds), \n",
    "                                     'T': np.array(T_prev),\n",
    "                                     'init_inds': init_inds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.clear()\n",
    "ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "ax.view_init(elev=16., azim=-50)\n",
    "fig.add_axes(ax)\n",
    "plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "plt.xlim(-1.8, 1.8)\n",
    "plt.ylim(-1.8, 1.8)\n",
    "ax.set_zlim(-1.8, 1.8)\n",
    "ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "T_prev_tmp = T_prev.at[init_inds].set(T_prev[init_inds]-init_trials)\n",
    "sampled_inds_tmp = np.where(np.absolute(np.array(T_prev_tmp)).astype(int) > 0)[0]\n",
    "scat = ax.scatter(X_expt_orig[sampled_inds_tmp, 0], \n",
    "            X_expt_orig[sampled_inds_tmp, 1],\n",
    "            X_expt_orig[sampled_inds_tmp, 2], marker='o', c=T_prev_tmp[sampled_inds_tmp], s=20, cmap=cm.jet, vmin=0, vmax=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# savemat('performances_5step_p2_n259_adamw.mat', {'performance_stack': np.array(performance_stack),\n",
    "#                                                 'performance_stack_random': np.array(performance_stack_random)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61de6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.errorbar(np.mean(np.array(num_samples_stack), axis=0), np.mean(np.array(performance_stack), 0), \n",
    "             yerr=np.std(np.array(performance_stack), axis=0), fmt='o', ls='-', linewidth=4, elinewidth=2, label='Active Learning', c='tab:blue', alpha=0.3)\n",
    "plt.errorbar(np.mean(np.array(num_samples_stack), axis=0), np.mean(np.array(performance_stack_random), 0), \n",
    "             yerr=np.std(np.array(performance_stack_random), axis=0), fmt='o', ls='-', linewidth=4, elinewidth=2, label='Random Baseline', c='tab:orange', alpha=0.3)\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlabel('Number of Trials Sampled', fontsize=24)\n",
    "plt.ylabel(r'RMSE', fontsize=24)\n",
    "plt.legend(fontsize=20)\n",
    "# plt.ylim(0.36, 0.4)\n",
    "\n",
    "# plt.savefig('performances_5step_p2_n259_adamw.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5208611",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_true_t, w_true_nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combos = fitting.all_combos(np.arange(len(cells)))\n",
    "ws_full = []\n",
    "for i in range(len(cells)):\n",
    "    ws_full.append(np.array(ws[i]))\n",
    "\n",
    "ws_active = []\n",
    "ws_active += [np.array(w_final_t)]\n",
    "ws_active += [np.array(w_final_nt)]\n",
    "\n",
    "for j in all_combos:\n",
    "    combo = np.array(j)\n",
    "    if len(combo) != 0 and len(combo) != len(cells):\n",
    "        targets = combo\n",
    "        selectivities_full.append(fitting.selectivity_triplet(ws_full, targets))\n",
    "        selectivities_active.append(fitting.selectivity_triplet(ws_active, targets))\n",
    "        datasets.append((dataset, wnoise, estim, p, cells, cells[targets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24afa60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectivities_full, selectivities_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73133528",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7fbb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(selectivities_full)[:, 1]\n",
    "y = np.vstack((np.array(selectivities_full)[:, 0], np.array(selectivities_active)[:, 0])).T\n",
    "\n",
    "lines = []\n",
    "for i, j in zip(x, y):\n",
    "    pair = [(i, j[0]), (i, j[1])]\n",
    "    lines.append(pair)\n",
    "\n",
    "linecoll = matcoll.LineCollection(lines, colors='k')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, [i for (i,j) in y], 's', c='tab:blue', markersize = 10, alpha=0.8, label='Full Dataset')\n",
    "ax.plot(x, [j for (i,j) in y], 'o', c='tab:red', markersize = 10, alpha=0.8, label='Subsampled Dataset')\n",
    "ax.add_collection(linecoll)\n",
    "ax.plot(np.linspace(-0.01, 1.01, 100), np.linspace(-0.01, 1.01, 100), linestyle='--', c='k')\n",
    "ax.set_xlabel('Single-Electrode Selectivity', fontsize=22)\n",
    "ax.set_ylabel('Triplet Selectivity', fontsize=22)\n",
    "ax.tick_params('both', labelsize=18)\n",
    "ax.legend(fontsize=16)\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "plt.savefig('fig2_NER.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1804235",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('selectivities_full.npy', np.array(selectivities_full))\n",
    "np.save('selectivities_active.npy', np.array(selectivities_active))\n",
    "np.save('datasets.npy', np.array(datasets, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a9bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 13:09:58) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a49c36f17b22be80bca1a531e420dccca9dd908a69799277bc977a4a0d3e51d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
