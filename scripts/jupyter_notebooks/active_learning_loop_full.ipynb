{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f723e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # export OMP_NUM_THREADS=1\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" # export OPENBLAS_NUM_THREADS=1\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\" # export MKL_NUM_THREADS=1\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\" # export VECLIB_MAXIMUM_THREADS=1\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" # export NUMEXPR_NUM_THREADS=1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import src.fitting as fitting\n",
    "import src.multielec_utils as mutils\n",
    "import statsmodels.api as sm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import multiprocessing as mp\n",
    "from itertools import product\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# jax.config.update('jax_platform_name', 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad71c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_probs(x, w):\n",
    "    # w : site weights, n x d\n",
    "    # x : current levels, c x d\n",
    "    site_activations = jnp.dot(w, jnp.transpose(x)) # dimensions: n x c\n",
    "    p_sites = jax.nn.sigmoid(site_activations) # dimensions : n x c\n",
    "    p = 1 - jnp.prod(1 - p_sites, 0)  # dimensions: c\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d15748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path definitions\n",
    "ANALYSIS_BASE = \"/Volumes/Analysis\"\n",
    "MATFILE_BASE = \"/Volumes/Scratch/Users/praful/triplet_gsort_matfiles_20220420\"\n",
    "gsort_path = None\n",
    "gsort_path_1elec = \"/Volumes/Scratch/Users/praful/single_gsort_v2_30um_periphery-affinity_cosine\"\n",
    "\n",
    "dataset = \"2020-10-06-7\"\n",
    "estim = \"data003/data003-all\"\n",
    "estim_1elec = \"data001\"\n",
    "wnoise = \"kilosort_data000/data000\"\n",
    "electrical_path = os.path.join(ANALYSIS_BASE, dataset, estim)\n",
    "vis_datapath = os.path.join(ANALYSIS_BASE, dataset, wnoise)\n",
    "\n",
    "p = 1\n",
    "\n",
    "X_expt_orig = mutils.get_stim_amps_newlv(electrical_path, p)\n",
    "w_true = jnp.array([[-5.98518703, -5.73843676, -1.36037982, -0.05980741],\n",
    "       [-5.98518703, -2.28047189, -2.93318102, -4.31001908],\n",
    "       [-5.98518703,  5.39557745,  1.95279497,  1.8031558 ],\n",
    "       [-5.98518703, -0.25671708,  2.89097144,  3.80746902]])\n",
    "# w_true = jnp.array([[-5.68501006,  2.44477339,  3.23685565,  2.75812431],\n",
    "#        [-5.66911426, -2.57285102, -3.49945348, -2.8179713 ]])\n",
    "\n",
    "X = jnp.array(sm.add_constant(X_expt_orig, has_constant='add'))\n",
    "p_true = activation_probs(X, w_true) # prob with each current level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.clear()\n",
    "ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "plt.xlim(-1.8, 1.8)\n",
    "plt.ylim(-1.8, 1.8)\n",
    "ax.set_zlim(-1.8, 1.8)\n",
    "ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "scat = ax.scatter(X_expt_orig[:, 0], \n",
    "            X_expt_orig[:, 1],\n",
    "            X_expt_orig[:, 2], marker='o', c=p_true, s=20, alpha=0.8, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3dd0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_spikes(p_true, t):\n",
    "    p_true, t = np.array(p_true), np.array(t).astype(int)\n",
    "    \n",
    "    p_empirical = []\n",
    "    for i in range(len(p_true)):\n",
    "        if t[i] == 0:\n",
    "            p_empirical += [0.5]\n",
    "        \n",
    "        else:\n",
    "            p_empirical += [np.mean(np.random.choice(np.array([0, 1]), \n",
    "                                                 p=np.array([1-p_true[i], p_true[i]]), \n",
    "                                                 size=t[i]))]\n",
    "        \n",
    "    return p_empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood(w, x, y, trials, l2_reg=0):\n",
    "    # x : current levels, c x d\n",
    "    # w : site weights, n x d\n",
    "    # y : empirical probability for each current level, c\n",
    "    # trials: number of trials at each current level, c\n",
    "    # l2_reg: l2 regularization penalty\n",
    "    # w = w.reshape(-1, x.shape[-1])  # dimensions: n x d\n",
    "    \n",
    "    p_model = activation_probs(x, w) # dimensions: c\n",
    "    p_model = jnp.clip(p_model, a_min=1e-5, a_max=1-1e-5)\n",
    "\n",
    "    trials = trials.astype(int)\n",
    "    \n",
    "    nll = -jnp.sum(trials * y * jnp.log(p_model) + trials * (1 - y) * jnp.log(1 - p_model))\n",
    "\n",
    "    penalty = l2_reg/2 * jnp.linalg.norm(w)**2\n",
    "\n",
    "    return nll + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeea455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_w(x, w, y, trials, l2_reg=0, zero_prob=0.01, step_size=0.0001, n_steps=100, wtol=1e-4):\n",
    "\n",
    "    m = len(w)\n",
    "    z = 1 - (1 - zero_prob)**(1/m)\n",
    "\n",
    "    @jax.jit\n",
    "    def update(x, w, y, trials, l2_reg):\n",
    "        grads = jax.grad(neg_log_likelihood)(w, x, y, trials, l2_reg=l2_reg)\n",
    "        return grads\n",
    "\n",
    "    losses = []\n",
    "    prev_w = w\n",
    "    for step in range(n_steps):\n",
    "        grad = update(x, w, y, trials, l2_reg)\n",
    "        w = w - step_size * grad\n",
    "        losses += [neg_log_likelihood(w, x, y, trials, l2_reg=l2_reg)]\n",
    "        w = w.at[:, 0].set(jnp.minimum(w[:, 0], np.log(z/(1-z))))\n",
    "\n",
    "        # print(step, jnp.linalg.norm(w - prev_w) / len(w.ravel()), jnp.linalg.norm(grad) / len(w.ravel()))\n",
    "        if jnp.linalg.norm(w - prev_w) / len(w.ravel()) <= wtol:\n",
    "            break\n",
    "        prev_w = w\n",
    "        \n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_info(x, w, y, t):\n",
    "    # x : current levels, c x d\n",
    "    # w : site weights, n x d\n",
    "    # y : empirical probability for each current level, c\n",
    "    # t: number of trials for each current level, c\n",
    "    \n",
    "    p_model = jnp.clip(activation_probs(x, w), a_min=1e-5, a_max=1-1e-5) # c\n",
    "    I_p = jnp.diag(t / (p_model * (1 - p_model)))   # c x c\n",
    "    J = jax.jacfwd(activation_probs, argnums=1)(x, w).reshape((len(x), w.shape[0]*w.shape[1]))\n",
    "    I_w = jnp.dot(jnp.dot(J.T, I_p), J) / len(x)\n",
    "    \n",
    "    loss = jnp.trace(J @ (jnp.linalg.inv(I_w) @ J.T))\n",
    "    # sign, logdet = jnp.linalg.slogdet(I_w)\n",
    "    # loss = -sign * logdet\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_proj_simplex(v, s=1):\n",
    "    \"\"\" Compute the Euclidean projection on a positive simplex\n",
    "    Solves the optimisation problem (using the algorithm from [1]):\n",
    "        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0 \n",
    "    Parameters\n",
    "    ----------\n",
    "    v: (n,) numpy array,\n",
    "       n-dimensional vector to project\n",
    "    s: int, optional, default: 1,\n",
    "       radius of the simplex\n",
    "    Returns\n",
    "    -------\n",
    "    w: (n,) numpy array,\n",
    "       Euclidean projection of v on the simplex\n",
    "    Notes\n",
    "    -----\n",
    "    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n",
    "    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n",
    "    However, this implementation still easily scales to millions of dimensions.\n",
    "    References\n",
    "    ----------\n",
    "    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n",
    "        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n",
    "        International Conference on Machine Learning (ICML 2008)\n",
    "        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n",
    "    \"\"\"\n",
    "    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n",
    "    n, = v.shape  # will raise ValueError if v is not 1-D\n",
    "    # check if we are already on the simplex\n",
    "    if v.sum() == s and np.alltrue(v >= 0):\n",
    "        # best projection: itself!\n",
    "        return v\n",
    "    # get the array of cumulative sums of a sorted (decreasing) copy of v\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    # get the number of > 0 components of the optimal solution\n",
    "    rho = np.nonzero(u * np.arange(1, n+1) > (cssv - s))[0][-1]\n",
    "    # compute the Lagrange multiplier associated to the simplex constraint\n",
    "    theta = (cssv[rho] - s) / (rho + 1.0)\n",
    "    # compute the projection by thresholding v using theta\n",
    "    w = (v - theta).clip(min=0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74fbbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_fisher(x, w, y, t_prev, t, reg=0, step_size=0.001, n_steps=100, step_cnt_decrement=5, reltol=-np.inf, T_budget=5000):\n",
    "\n",
    "    @jax.jit\n",
    "    def update(x, w, y, t_prev, t):\n",
    "        fisher_lambda = lambda t, x, w, y, t_prev: fisher_info(x, w, y, t_prev + jnp.absolute(t)) + reg * (jnp.absolute(jnp.sum(jnp.absolute(t)) - T_budget))\n",
    "        grads = jax.grad(fisher_lambda)(t, x, w, y, t_prev)\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    losses = []\n",
    "    last_t = t\n",
    "    for step in range(n_steps):\n",
    "        grad = update(x, w, y, t_prev, t)\n",
    "        t = t - step_size * grad\n",
    "    \n",
    "        losses += [[fisher_info(x, w, y, t_prev + jnp.absolute(t)), \n",
    "                    jnp.sum(jnp.absolute(t)),\n",
    "                    fisher_info(x, w, y, t_prev + jnp.absolute(t)) + reg * (jnp.absolute(jnp.sum(jnp.absolute(t)) - T_budget))]]\n",
    "\n",
    "        # curr_loss = fisher_info(x, w, y, t_prev + jnp.absolute(t)) + reg * jnp.sum(jnp.absolute(t))\n",
    "        # last_loss = fisher_info(x, w, y, t_prev + jnp.absolute(last_t)) + reg * jnp.sum(jnp.absolute(last_t))\n",
    "        # rel_decrease = jnp.absolute(curr_loss - last_loss) / last_loss\n",
    "        # scaled_grad = jnp.linalg.norm(grad) / len(t)\n",
    "        # # print(step, jnp.absolute(curr_loss - last_loss) / last_loss, jnp.linalg.norm(grad) / len(t))\n",
    "        # if rel_decrease <= reltol:\n",
    "        #     break\n",
    "\n",
    "        # last_t = t\n",
    "        if step % step_cnt_decrement == 0:\n",
    "            step_size = step_size * 0.95\n",
    "        \n",
    "    return np.array(losses), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search_fisher(reg_array, low, high, x, w, y, t_prev, t, T_budget, budget_tol=0.1, step_size=0.001, n_steps=100, step_cnt_decrement=5, reltol=-np.inf):\n",
    "    losses, t = optimize_fisher(x, w, y, t_prev, t, reg=reg_array[high], step_size=step_size, n_steps=n_steps, step_cnt_decrement=step_cnt_decrement, reltol=reltol)\n",
    "    budget_high = jnp.sum(jnp.round(jnp.absolute(t), 0))\n",
    "    print(reg_array[high], budget_high)\n",
    "\n",
    "    if budget_high <= (1 + budget_tol) * T_budget and budget_high >= (1 - budget_tol) * T_budget:\n",
    "        return losses, t, high\n",
    "\n",
    "    losses, t = optimize_fisher(x, w, y, t_prev, t, reg=reg_array[low], step_size=step_size, n_steps=n_steps, step_cnt_decrement=step_cnt_decrement, reltol=reltol)\n",
    "    budget_low = jnp.sum(jnp.round(jnp.absolute(t), 0))\n",
    "    print(reg_array[low], budget_low)\n",
    "\n",
    "    if budget_low <= (1 + budget_tol) * T_budget and budget_low >= (1 - budget_tol) * T_budget:\n",
    "        return losses, t, low\n",
    "\n",
    "    if budget_high > T_budget and budget_low < T_budget:\n",
    "        mid = (high + low) // 2\n",
    "\n",
    "        losses, t = optimize_fisher(x, w, y, t_prev, t, reg=reg_array[mid], step_size=step_size, n_steps=n_steps, step_cnt_decrement=step_cnt_decrement, reltol=reltol)\n",
    "        budget_mid = jnp.sum(jnp.round(jnp.absolute(t), 0))\n",
    "        print(reg_array[mid], budget_mid)\n",
    "        if budget_mid <= (1 + budget_tol) * T_budget and budget_mid >= (1 - budget_tol) * T_budget:\n",
    "            return losses, t, mid\n",
    "\n",
    "        elif budget_mid > T_budget:\n",
    "            return binary_search_fisher(reg_array, low+1, mid-1, x, w, y, t_prev, t, T_budget, budget_tol=budget_tol, step_size=step_size, n_steps=n_steps, \n",
    "                                        step_cnt_decrement=step_cnt_decrement, reltol=reltol)\n",
    "        \n",
    "        else:\n",
    "            return binary_search_fisher(reg_array, mid+1, high-1, x, w, y, t_prev, t, T_budget, budget_tol=budget_tol, step_size=step_size, n_steps=n_steps, \n",
    "                                        step_cnt_decrement=step_cnt_decrement, reltol=reltol)\n",
    "\n",
    "    else:\n",
    "        return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_AL(X, w_meas, p_true):\n",
    "    probs_pred = activation_probs(X, w_meas)\n",
    "    RMSE = jnp.sqrt(jnp.sum((probs_pred - p_true)**2) / len(X))\n",
    "\n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3593e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "total_budget = 10000\n",
    "num_iters = 5\n",
    "budget = int(total_budget / num_iters)\n",
    "reg = 10#np.flip(np.logspace(-5, 3, 100000, base=2))\n",
    "ms = [2, 3, 4, 5]\n",
    "num_restarts = 100\n",
    "l2_reg = 0.01\n",
    "R2_thresh = 0.02\n",
    "\n",
    "init_size = 200\n",
    "init_trials = 5\n",
    "\n",
    "performance_stack = []\n",
    "performance_stack_random = []\n",
    "num_samples_stack = []\n",
    "\n",
    "for restart in range(num_restarts):\n",
    "    print('Restart', restart + 1)\n",
    "    # Initialize amplitudes\n",
    "    init_inds = np.random.choice(len(X), replace=False, size=init_size)\n",
    "\n",
    "    # Initialize trials\n",
    "    T_prev = jnp.zeros(len(X_expt_orig))\n",
    "    T_prev = T_prev.at[init_inds].set(init_trials)\n",
    "    T_prev_random = jnp.copy(T_prev)\n",
    "\n",
    "    p_empirical = jnp.array(sample_spikes(p_true, T_prev))\n",
    "    p_empirical_random = jnp.copy(p_empirical)\n",
    "\n",
    "    # Initialize weights\n",
    "\n",
    "    w_inits = []\n",
    "    for m in ms:\n",
    "        w_init = jnp.array(np.random.normal(size=(m, X.shape[1])))\n",
    "        w_inits.append(w_init)\n",
    "\n",
    "    w_inits_random = w_inits.copy()\n",
    "\n",
    "    performances = []\n",
    "    performances_random = []\n",
    "    num_samples = []\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    while True:\n",
    "        # reg = regs[cnt]\n",
    "        num_samples.append(np.sum(np.absolute(np.array(T_prev)).astype(int)))\n",
    "        sampled_inds = np.where(np.absolute(np.array(T_prev)).astype(int) > 0)[0]\n",
    "\n",
    "        # fig = plt.figure()\n",
    "        # fig.clear()\n",
    "        # ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "        # fig.add_axes(ax)\n",
    "        # plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "        # plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "        # plt.xlim(-1.8, 1.8)\n",
    "        # plt.ylim(-1.8, 1.8)\n",
    "        # ax.set_zlim(-1.8, 1.8)\n",
    "        # ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "        # scat = ax.scatter(X_expt_orig[sampled_inds, 0], \n",
    "        #             X_expt_orig[sampled_inds, 1],\n",
    "        #             X_expt_orig[sampled_inds, 2], marker='o', c=p_empirical[sampled_inds], s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "\n",
    "        # plt.show()\n",
    "\n",
    "        sampled_inds_random = np.where(np.absolute(np.array(T_prev_random)).astype(int) > 0)[0]\n",
    "\n",
    "        # fig = plt.figure()\n",
    "        # fig.clear()\n",
    "        # ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "        # fig.add_axes(ax)\n",
    "        # plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "        # plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "        # plt.xlim(-1.8, 1.8)\n",
    "        # plt.ylim(-1.8, 1.8)\n",
    "        # ax.set_zlim(-1.8, 1.8)\n",
    "        # ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "        # scat = ax.scatter(X_expt_orig[sampled_inds_random , 0], \n",
    "        #             X_expt_orig[sampled_inds_random , 1],\n",
    "        #             X_expt_orig[sampled_inds_random , 2], marker='o', c=p_empirical_random[sampled_inds_random], s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "\n",
    "        # plt.show()\n",
    "        \n",
    "        ybar = jnp.sum(p_empirical * T_prev) / jnp.sum(T_prev)\n",
    "        beta_null = jnp.log(ybar / (1 - ybar))\n",
    "        null_weights = jnp.concatenate((jnp.array([beta_null]), jnp.zeros(X_expt_orig.shape[-1])))\n",
    "        nll_null = neg_log_likelihood(null_weights[None, :], X, p_empirical, T_prev, l2_reg=l2_reg)\n",
    "\n",
    "        # weights_BIC = []\n",
    "        # BICs = []\n",
    "        # Optimize w\n",
    "        for i in range(len(ms)):\n",
    "\n",
    "            losses, w_final = optimize_w(X, w_inits[i], p_empirical, T_prev, l2_reg=0.01, step_size=0.001, n_steps=3500)\n",
    "            p_pred = activation_probs(X, w_final)\n",
    "\n",
    "            # BICs.append(len(w_final.ravel()) * jnp.log(len(X)) + 2 * losses[-1])\n",
    "            # weights_BIC.append(w_final)\n",
    "\n",
    "            # fig = plt.figure()\n",
    "            # fig.clear()\n",
    "            # ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "            # fig.add_axes(ax)\n",
    "            # plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "            # plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "            # plt.xlim(-1.8, 1.8)\n",
    "            # plt.ylim(-1.8, 1.8)\n",
    "            # ax.set_zlim(-1.8, 1.8)\n",
    "            # ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "            # scat = ax.scatter(X_expt_orig[:, 0], \n",
    "            #             X_expt_orig[:, 1],\n",
    "            #             X_expt_orig[:, 2], marker='o', c=p_pred, s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "\n",
    "            # plt.show()\n",
    "\n",
    "            # plt.figure()\n",
    "            # plt.plot(losses)\n",
    "            # plt.axhline(neg_log_likelihood(w_true, X, p_empirical, T_prev), linestyle='--', c='k')\n",
    "            # plt.show()\n",
    "\n",
    "            w_inits[i] = w_final\n",
    "\n",
    "            if i == 0:\n",
    "                last_R2 = 1 - losses[-1] / nll_null\n",
    "                last_opt = w_final\n",
    "\n",
    "            else:\n",
    "                new_R2 = 1 - losses[-1] / nll_null\n",
    "                if new_R2 - last_R2 <= R2_thresh:\n",
    "                    break\n",
    "                else:\n",
    "                    last_R2 = new_R2\n",
    "                    last_opt = w_final\n",
    "\n",
    "        # w_final = weights_BIC[jnp.argmin(jnp.array(BICs))]\n",
    "        w_final = last_opt\n",
    "        performance = get_performance_AL(X, w_final, p_true)\n",
    "        performances.append(performance)\n",
    "\n",
    "        # print(jnp.array(BICs), w_final)\n",
    "\n",
    "\n",
    "        ybar = jnp.sum(p_empirical_random * T_prev_random) / jnp.sum(T_prev_random)\n",
    "        beta_null = jnp.log(ybar / (1 - ybar))\n",
    "        null_weights = jnp.concatenate((jnp.array([beta_null]), jnp.zeros(X_expt_orig.shape[-1])))\n",
    "        nll_null = neg_log_likelihood(null_weights[None, :], X, p_empirical_random, T_prev_random, l2_reg=l2_reg)\n",
    "        # weights_BIC_random = []\n",
    "        # BICs_random = []\n",
    "        # Optimize w\n",
    "        for i in range(len(ms)):\n",
    "\n",
    "            losses_random, w_final_random = optimize_w(X, w_inits_random[i], p_empirical_random, T_prev_random, l2_reg=0.01, step_size=0.001, n_steps=3500)\n",
    "            p_pred_random = activation_probs(X, w_final_random)\n",
    "\n",
    "            # BICs_random.append(len(w_final_random.ravel()) * jnp.log(len(X)) + 2 * losses_random[-1])\n",
    "            # weights_BIC_random.append(w_final_random)\n",
    "\n",
    "            # fig = plt.figure()\n",
    "            # fig.clear()\n",
    "            # ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "            # fig.add_axes(ax)\n",
    "            # plt.xlabel(r'$I_1$ ($\\mu$A)', fontsize=16)\n",
    "            # plt.ylabel(r'$I_2$ ($\\mu$A)', fontsize=16)\n",
    "            # plt.xlim(-1.8, 1.8)\n",
    "            # plt.ylim(-1.8, 1.8)\n",
    "            # ax.set_zlim(-1.8, 1.8)\n",
    "            # ax.set_zlabel(r'$I_3$ ($\\mu$A)', fontsize=16)\n",
    "\n",
    "            # scat = ax.scatter(X_expt_orig[:, 0], \n",
    "            #             X_expt_orig[:, 1],\n",
    "            #             X_expt_orig[:, 2], marker='o', c=p_pred_random, s=20, alpha=0.8, vmin=0, vmax=1)\n",
    "\n",
    "            # plt.show()\n",
    "\n",
    "            # plt.figure()\n",
    "            # plt.plot(losses_random)\n",
    "            # plt.axhline(neg_log_likelihood(w_true, X, p_empirical_random, T_prev_random), linestyle='--', c='k')\n",
    "            # plt.show()\n",
    "\n",
    "            w_inits_random[i] = w_final_random\n",
    "\n",
    "            if i == 0:\n",
    "                last_R2 = 1 - losses_random[-1] / nll_null\n",
    "                last_opt = w_final_random\n",
    "\n",
    "            else:\n",
    "                new_R2 = 1 - losses_random[-1] / nll_null\n",
    "                if new_R2 - last_R2 <= R2_thresh:\n",
    "                    break\n",
    "                else:\n",
    "                    last_R2 = new_R2\n",
    "                    last_opt = w_final_random\n",
    "\n",
    "        # w_final_random = weights_BIC_random[jnp.argmin(jnp.array(BICs_random))]\n",
    "        w_final_random = last_opt\n",
    "        performance_random = get_performance_AL(X, w_final_random, p_true)\n",
    "        performances_random.append(performance_random)\n",
    "\n",
    "        # print(jnp.array(BICs_random), w_final_random)\n",
    "        print(performance, performance_random)\n",
    "\n",
    "        if cnt >= num_iters:\n",
    "            break\n",
    "\n",
    "        # explore = performance\n",
    "        # explore_batch = 0#int(explore * budget) * 2\n",
    "        # exploit_batch = budget - explore_batch\n",
    "\n",
    "        T_new_init = jnp.zeros(len(T_prev)) + 1\n",
    "        losses, t_final = optimize_fisher(X, w_final, p_empirical, T_prev, T_new_init, reg=reg, step_size=0.01, n_steps=1000, T_budget=budget)\n",
    "        # losses, t_final, reg_ind = binary_search_fisher(regs, 0, len(regs)-1, X, w_final, p_empirical, T_prev, T_new_init, budget, budget_tol=0.2, step_size=0.01, n_steps=1000)\n",
    "        # print(regs[reg_ind])\n",
    "\n",
    "        # fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        # axs[0].plot(losses[:, 0])\n",
    "        # axs[0].set_ylabel('Fisher Loss (A-optimality)')\n",
    "        # axs[1].plot(losses[:, 1])\n",
    "        # axs[1].set_ylabel('Total Trials')\n",
    "        # axs[2].plot(losses[:, 2])\n",
    "        # axs[2].set_ylabel('Regularized Loss, reg=' + str(reg))\n",
    "\n",
    "        # fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "        # plt.show()\n",
    "\n",
    "        # random_draws_explore = np.random.choice(len(X), size=explore_batch)\n",
    "        # T_new_explore = jnp.array(np.bincount(random_draws_explore, minlength=len(X)))\n",
    "\n",
    "        T_new = jnp.round(jnp.absolute(t_final), 0)#(t_final + T_new_explore), 0)\n",
    "        # print(jnp.sum(T_new))\n",
    "        # plt.figure()\n",
    "        # plt.plot(T_new)\n",
    "        # plt.show()\n",
    "\n",
    "        if jnp.sum(T_new) < budget:\n",
    "            random_extra = np.random.choice(len(X), size=int(budget - jnp.sum(T_new)))#, p=np.array(jnp.absolute(t_final)))\n",
    "            T_new_extra = jnp.array(np.bincount(random_extra, minlength=len(X))).astype(int)\n",
    "            T_new = T_new + T_new_extra\n",
    "            \n",
    "            # print(jnp.sum(T_new))\n",
    "            # plt.figure()\n",
    "            # plt.plot(T_new)\n",
    "            # plt.show()\n",
    "\n",
    "        p_new = jnp.array(sample_spikes(p_true, T_new))\n",
    "\n",
    "        p_tmp = (p_new * T_new + p_empirical * T_prev) / (T_prev + T_new)\n",
    "        T_tmp = T_prev + T_new\n",
    "        p_tmp = p_tmp.at[jnp.isnan(p_tmp)].set(0.5)\n",
    "\n",
    "        p_empirical = p_tmp\n",
    "        T_prev = T_tmp\n",
    "        # print(jnp.sum(T_tmp))\n",
    "\n",
    "        random_draws = np.random.choice(len(X), size=int(jnp.sum(T_new)))\n",
    "        T_new_random = jnp.array(np.bincount(random_draws, minlength=len(X))).astype(int)\n",
    "        p_new_random = jnp.array(sample_spikes(p_true, T_new_random))\n",
    "        \n",
    "        p_tmp_random = (p_new_random * T_new_random + p_empirical_random * T_prev_random) / (T_prev_random + T_new_random)\n",
    "        T_tmp_random = T_prev_random + T_new_random\n",
    "        p_tmp_random = p_tmp_random.at[jnp.isnan(p_tmp_random)].set(0.5)\n",
    "\n",
    "        p_empirical_random = p_tmp_random\n",
    "        T_prev_random = T_tmp_random\n",
    "\n",
    "        cnt += 1\n",
    "    \n",
    "    performance_stack.append(performances)\n",
    "    performance_stack_random.append(performances_random)\n",
    "    num_samples_stack.append(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61de6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.errorbar(np.mean(np.array(num_samples_stack), axis=0), np.mean(np.array(performance_stack), 0), \n",
    "             yerr=np.std(np.array(performance_stack), axis=0), fmt='o', ls='-', linewidth=4, elinewidth=2, label='Active Learning', c='tab:blue', alpha=0.5)\n",
    "plt.errorbar(np.mean(np.array(num_samples_stack), axis=0), np.mean(np.array(performance_stack_random), 0), \n",
    "             yerr=np.std(np.array(performance_stack_random), axis=0), fmt='o', ls='-', linewidth=4, elinewidth=2, label='Random Baseline', c='tab:orange', alpha=0.5)\n",
    "# plt.yscale('log')\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlabel('Number of Trials Sampled', fontsize=24)\n",
    "plt.ylabel(r'RMSE', fontsize=24)\n",
    "plt.legend(fontsize=20)\n",
    "# plt.ylim(0.36, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5208611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pvasi39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a49c36f17b22be80bca1a531e420dccca9dd908a69799277bc977a4a0d3e51d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
